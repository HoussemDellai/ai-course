{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Azure AI Search and OpenAI\n",
    "\n",
    "This code demonstrates how to work with RAG to give more context to the LLM/SLM models to get a more accurate answer. The code uses Azure AI Search to index the documents and Azure OpenAI's embedding model to generate embeddings/vectors for the documents.\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-ada-002 model\n",
    "+ Index the vector and non-vector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "## Create the resources\n",
    "\n",
    "Refer to the `README.md` file in the root folder to create the resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install tiktoken\n",
    "%pip install azure-search-documents\n",
    "%pip install azure-identity\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Azure AI Search and OpenAI\n",
    "\n",
    "Load environment variables from the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "    config = dotenv_values(\".env\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_chat_completions_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETIONS_DEPLOYMENT_NAME\")\n",
    "\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "embedding_vector_dimensions = os.getenv(\"EMBEDDING_VECTOR_DIMENSIONS\")\n",
    "\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_SERVICE_ADMIN_KEY\")\n",
    "search_index_name = os.getenv(\"SEARCH_INDEX_NAME\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Test connection to OpenAI ChatGPT\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=azure_openai_chat_completions_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you ?\"}\n",
    "    ])\n",
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of tokens in a text\n",
    "\n",
    "Like LLM models, Embedding models defines a `max input`. It is defined in number of `tokens`. The `max_input` for `text-embedding-3-large` is 8191 tokens. So we need to split the text into chunks of 8191 tokens or less. For that, you need to get the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name=\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# Test the function\n",
    "num_tokens_from_string(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI embedding model `text-embedding-3-large` has a limit of `8191` tokens per request.\n",
    "Before sending the files to the model, we need to split the text into chunks of less than `8191` tokens.\n",
    "Count the number of tokens in the sample files and show the files with more than `8191` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './data/azure-ai-docs/'\n",
    "i=0\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.md'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            tokens = num_tokens_from_string(content)\n",
    "            if tokens > 8191:\n",
    "                print(f'File {filename} has {tokens} tokens which is more than 8191 (max) tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming/cleaning the documents\n",
    "\n",
    "Later in this lab, we will proceed with markdown `.md` files. We will need to remove all special characters and markdown syntax from the files. The function `clean_markdown_content()` will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markdown_content(content):\n",
    "    # Remove links\n",
    "    link_pattern = r'\\[([^\\[]+)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(link_pattern, r'\\1', content)\n",
    "\n",
    "    # Remove images\n",
    "    image_pattern = r'\\!\\[([^\\[]*)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(image_pattern, '', content)\n",
    "\n",
    "    # Remove all occurrences of **\n",
    "    content = content.replace('**', '')\n",
    "    content = content.replace('\\n', '')\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the vector embedding for an input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_vector(text):\n",
    "\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=azure_openai_embedding_model,\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Test the function\n",
    "vector = get_embeddings_vector(\"Sample text\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file chunks\n",
    "\n",
    "This is where we split the markdown files in folder `./data/azure-ai-docs` into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_directory = './data/azure-ai-docs/'\n",
    "output_directory = './data/chunks/'\n",
    "# create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "chunk_index=0\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # Check if the file is a markdown file\n",
    "    if filename.endswith('.md'):\n",
    "        # Open the file\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            print(filename)\n",
    "            # Read the file content\n",
    "            content = file.read()\n",
    "            \n",
    "            # break if content doesn't contain title, description, ms.date and '##'\n",
    "            if 'title:' not in content or 'description:' not in content or 'ms.date:' not in content or '##' not in content:\n",
    "                print(f'File {filename} does not contain title, description, ms.date or ##')\n",
    "                continue\n",
    "\n",
    "            # Extract the title, description, and date\n",
    "            page_title = re.search(r'title: (.*)', content).group(1).replace('\"', '')\n",
    "            page_description = re.search(r'description: (.*)', content).group(1)\n",
    "            page_date = re.search(r'ms.date: (.*)', content).group(1)\n",
    "            \n",
    "            # Split the content into chunks based on '##'\n",
    "            chunks = content.split('\\n## ')[1:]  # Skip the first chunk as it contains the title, description, and date\n",
    "            \n",
    "            # Add the chunks to the list along with the title, description, and date\n",
    "            for chunk in chunks:\n",
    "                chunk_index=chunk_index + 1\n",
    "                chunk_content = clean_markdown_content(chunk.strip())\n",
    "                \n",
    "                if (num_tokens_from_string(chunk_content) > 8191):\n",
    "                    print(f'Chunk {chunk_index} in file {filename} has more than 8191 tokens')\n",
    "                    break\n",
    "\n",
    "                vector = get_embeddings_vector(chunk_content)\n",
    "                \n",
    "                chunk = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    'page_title': page_title,\n",
    "                    'page_description': page_description,\n",
    "                    'page_date': page_date,\n",
    "                    'chunk_title': chunk.split('\\n')[0],  # The first line after '##' is the title of the chunk\n",
    "                    'chunk_content': chunk_content,  # Remove leading and trailing whitespaces\n",
    "                    'vector': vector\n",
    "                }\n",
    "                \n",
    "                chunk_file_name = f'chunk_{chunk_index}_{page_title}.json'.replace('?', '').replace(':', '').replace(\"'\", '').replace('|', '').replace('/', '').replace('\\\\', '')\n",
    "\n",
    "                # write chunk into JSON file into output directory\n",
    "                with open(f'{output_directory}/{chunk_file_name}', 'w') as f:\n",
    "                    json.dump(chunk, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the length of the embedding vector will be `1536` for `text-embedding-3-small` or `3072` for `text-embedding-3-large`. You can reduce the dimensions of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SemanticField\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=azure_search_service_endpoint, \n",
    "    index_name=search_index_name, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# create search index\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SearchableField(name=\"page_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"page_description\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"page_date\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=3072, #1536,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"page_title\"),\n",
    "        # keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        content_fields=[SemanticField(field_name=\"chunk_content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "# Create the search index with the semantic settings\n",
    "search_index = SearchIndex(name=search_index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = search_index_client.create_or_update_index(search_index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you ned to delete an index, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index\n",
    "search_index_client.delete_index(search_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload chunks/documents to Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, index_name=search_index_name, credential=credential)\n",
    "\n",
    "# for each json file in ./data/chunks/ folder, load the json document and upload it to the search index\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(output_directory, filename), 'r') as file:\n",
    "            document = json.load(file)\n",
    "\n",
    "            result = search_client.upload_documents(documents=document)\n",
    "            print(f\"Upload of {filename} succeeded: { result[0].succeeded }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search\n",
    "\n",
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"rag\"  \n",
    "\n",
    "embedding = get_embeddings_vector(query)\n",
    "\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"page_title\", \"page_date\", \"chunk_title\", \"chunk_content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:\n",
    "    print(f\"Page Date: {result['page_date']}\")  \n",
    "    print(f\"Page Title: {result['page_title']}\")  \n",
    "    print(f\"Chunk Title: {result['chunk_title']}\")  \n",
    "    print(f\"Chunk Content: {result['chunk_content']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user query\n",
    "\n",
    "This is where we will use the Azure AI Search to search for documents similar to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=azure_openai_chat_completions_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for an AI learner.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the LLM models supported by Azure ?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": azure_search_service_endpoint,\n",
    "                    \"index_name\": search_index_name,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": azure_search_service_admin_key,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
